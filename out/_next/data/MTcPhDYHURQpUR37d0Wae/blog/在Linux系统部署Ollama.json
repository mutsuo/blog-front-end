{"pageProps":{"data":{"id":"在Linux系统部署Ollama","date":"April 26, 2024","title":"Linux快速部署大语言模型LLaMa3，Web可视化j交互（Ollama+Open Web UI）","htmlContent":"<!doctype html>\n<html lang=\"en\">\n  <head>\n    <meta charset=\"utf-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n  </head>\n  <body>\n    <h1>介绍</h1>\n    <p>本文将介绍使用开源工具Ollama(60.6k⭐)部署LLaMa大模型，以及使用Open WebUI搭建前端Web交互界面的方法。</p>\n    <p>我们先来过一遍几个相关的概念，对这块比较熟悉的朋友可跳过。</p>\n    <h2>大规模语言模型</h2>\n    <p>大规模语言模型（Large Language Models, LLMs），顾名思义是指在大量语料数据的基础上训练成的模型，能够模拟人类的语言风格生成较为生动的文本。这类模型的主要特征有：</p>\n    <ul>\n      <li>规模大：训练所使用的数据量非常庞大，有时超过1000亿个参数。</li>\n      <li>复杂性高：模型结构比较复杂</li>\n      <li>具有较好的上下文理解能力：大规模语言模型可以理解文本的上下文和细微差别</li>\n    </ul>\n    <h2>LLaMa</h2>\n    <p>LLaMA是一种大规模语言模型，由Meta AI基于Transformer深度学习框架开发。该模型旨在生成各种风格的高质量文本（例如创意写作、对话甚至诗歌），能够胜任以下工作：</p>\n    <ul>\n      <li>自然语言处理（NLP）：理解和生成自然语言。</li>\n      <li>机器学习：根据数据和算法学习新的信息和技能。</li>\n      <li>对话生成：可以与用户进行对话，并根据情况生成合适的回应。</li>\n    </ul>\n    <h2>Ollama</h2>\n    <blockquote>\n      <p>官网：<a href=\"https://ollama.com/\">Ollama</a></p>\n      <p>API文档：<a href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\">ollama/docs/api.md at main · ollama/ollama (github.com)</a></p>\n      <p>支持的模型列表：<a href=\"https://ollama.com/library\">library</a></p>\n    </blockquote>\n    <p>一款可以快速部署大模型的工具。</p>\n    <h2>Open WebUI</h2>\n    <blockquote>\n      <p>官网：<a href=\"https://openwebui.com/\">Open WebUI</a></p>\n      <p>相关介绍及源码：<a href=\"https://github.com/open-webui/open-webui\">open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI) (github.com)</a></p>\n    </blockquote>\n    <p>Open WebUI 是一个可视化的Web交互环境，它拥有清新简约的UI风格，具有可扩展、功能丰富、用户友好、自托管的特点，可以完全离线运行。它支持各种 LLM 运行程序，包括 Ollama 和 OpenAI 兼容的 API。</p>\n    <h1>部署服务</h1>\n    <p>本文介绍的方法使用于Linux系统，同样适用于Windows系统的WSL（安装方法可参见我的<a href=\"https://blog.csdn.net/mustuo/article/details/133960230?\">这篇文章</a>）。</p>\n    <h2>部署Ollama</h2>\n    <p>1、下载Ollama</p>\n    <p>Linux系统的安装命令如下：</p>\n    <pre><code class=\"hljs language-shell\">curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>\n    <p>※此外<a href=\"https://ollama.com/download\">官方</a>还提供了macOS和Windows的下载方式。</p>\n    <p>2、下载llama3模型</p>\n    <pre><code class=\"hljs language-shell\">ollama run llama3\n</code></pre>\n    <p>※在<a href=\"https://ollama.com/blog/llama3\">这里</a>可以看到该命令的相关介绍。</p>\n    <p>上述命令将自动拉取模型，并进行sha256验签。处理完毕后自动进入llama3的运行环境，可以使用中文或英文进行提问，ctrl+D退出。</p>\n    <p>3、配置服务</p>\n    <p>为使外网环境能够访问到服务，需要对HOST进行配置。</p>\n    <p>打开配置文件：<code>vim /etc/systemd/system/ollama.service</code>，根据情况修改变量<code>Environment</code>：</p>\n    <ul>\n      <li>服务器环境下：<code>Environment=\"OLLAMA_HOST=0.0.0.0:11434\"</code></li>\n      <li>虚拟机环境下：<code>Environment=\"OLLAMA_HOST=服务器内网IP地址:11434\"</code></li>\n    </ul>\n    <p>3、启动服务</p>\n    <p>启动服务的命令：<code>ollama serve</code></p>\n    <p>首次启动可能会出现以下两个提示：</p>\n    <blockquote>\n      <p>Couldn't find '/home/用户名/.ollama/id_ed25519'. Generating new private key.</p>\n    </blockquote>\n    <p>该提示表示文件系统中不存在ssh私钥文件，此时命令将自动帮我们生成该文件，并在命令行中打印相应的公钥。</p>\n    <blockquote>\n      <p>Error: listen tcp 127.0.0.1:11434: bind: address already in use</p>\n    </blockquote>\n    <p>看到该提示，大概率服务已在运行中，可以通过<code>netstat -tulpn | grep 11434</code>命令进行确认。</p>\n    <ul>\n      <li>若命令输出的最后一列包含“ollama”字样，则表示服务已启动，无需做额外处理。</li>\n      <li>否则，可尝试执行下列命令重启ollama：</li>\n    </ul>\n    <pre><code class=\"hljs language-shell\"><span class=\"hljs-meta prompt_\"># </span><span class=\"bash\">ubuntu/debian</span>\r\nsudo apt update\r\nsudo apt install lsof\r\nstop ollama\r\nlsof -i :11434\r\nkill &#x3C;PID>\r\nollama serve\r<span class=\"hljs-meta prompt_\">\n\r\n# </span><span class=\"bash\">centos</span>\r\nsudo yum update\r\nsudo yum install lsof\r\nstop ollama\r\nlsof -i :11434\r\nkill &#x3C;PID>\r\nollama serve\n</code></pre>\n    <p>如果您使用的是MacOS，可在<a href=\"https://github.com/ollama/ollama/issues/707\">🔗这里</a>找到解决方法。</p>\n    <p>4、在外网环境验证连接</p>\n    <p>方法一：执行<code>curl http://ip:11434</code>命令，若返回“Ollama is running”，则表示连接正常。</p>\n    <p>方法二：在浏览器访问<a href=\"http://ip:11434%EF%BC%8C%E8%8B%A5%E9%A1%B5%E9%9D%A2%E6%98%BE%E7%A4%BA%E6%96%87%E6%9C%AC%E2%80%9COllama\">http://ip:11434，若页面显示文本“Ollama</a> is running”，则表示连接正常。</p>\n    <h2>常用命令</h2>\n    <p>1、进入llama3运行环境：<code>ollama run llama3</code></p>\n    <p>2、启动服务：<code>ollama serve</code></p>\n    <p>3、重启ollama</p>\n    <pre><code class=\"hljs language-shell\">systemctl daemon-reload\r\nsystemctl restart ollama\n</code></pre>\n    <p>4、重启ollama服务</p>\n    <pre><code class=\"hljs language-shell\"><span class=\"hljs-meta prompt_\"># </span><span class=\"bash\">ubuntu/debian</span>\r\nsudo apt update\r\nsudo apt install lsof\r\nstop ollama\r\nlsof -i :11434\r\nkill &#x3C;PID>\r\nollama serve\r<span class=\"hljs-meta prompt_\">\n\r\n# </span><span class=\"bash\">centos</span>\r\nsudo yum update\r\nsudo yum install lsof\r\nstop ollama\r\nlsof -i :11434\r\nkill &#x3C;PID>\r\nollama serve\n</code></pre>\n    <p>5、确认服务端口状态：<code>netstat -tulpn | grep 11434</code></p>\n    <h1>部署Open WebUI</h1>\n    <p>1、下载Open WebUI</p>\n    <p>Open WebUI基于docker部署，docker的安装方法可以参考<a href=\"https://zhuanlan.zhihu.com/p/651148141\">这篇知乎文章</a>。</p>\n    <p>Open WebUI既可以部署在服务端，也可以部署在客户端：</p>\n    <pre><code class=\"hljs language-shell\"><span class=\"hljs-meta prompt_\"># </span><span class=\"bash\">若部署在客户端，执行：</span>\r\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\r<span class=\"hljs-meta prompt_\">\n\r\n# </span><span class=\"bash\">若部署在服务端，执行：</span>\r\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n</code></pre>\n    <p>如果您的机器在国内，建议将<code>--restart</code>的参数值替换为<code>ghcr.nju.edu.cn/open-webui/open-webui:main</code>，下载速度会快非常多（见up主小杨生存日记的<a href=\"https://www.bilibili.com/read/cv32462618/\">这篇文章</a>）。</p>\n    <p>2、检查相关配置</p>\n    <p>下载完之后，就可以在浏览器访问了，地址为<code>http://loacalhost:3000</code>（客户端部署）或<code>http://服务器ip:3000</code>。</p>\n    <p>页面加载完成后（这个过程可能需要一些时间），新注册一个账号并登录。</p>\n    <p>登录之后，点击页面顶端的齿轮⚙图标进入设置：</p>\n    <ol>\n      <li>侧边导航栏-General，将语言设置为中文</li>\n      <li>侧边导航栏-连接，若“Ollama 基础 URL”这一项为<code>http://host.docker.internal:11434</code>，则表示ollama服务正常且连接成功；如果是空的，则需要回头检查一下ollama服务了</li>\n      <li>侧边导航栏-模型，一般会自动拉取ollama服务上部署好的模型，可选模型参看<a href=\"https://ollama.com/library/llama3\">官方的这篇文档</a></li>\n      <li>其它的项目根据需要设置即可</li>\n    </ol>\n    <p>3、选择模型</p>\n    <p>在顶端下拉框选择好模型，就可以开始提问啦！</p>\n    <h1>参考文章</h1>\n    <ul>\n      <li><a href=\"https://juejin.cn/post/7359470175761350690\">macOS + Ollama + Enchanted，本地部署最新 Llama3 - 掘金 (juejin.cn)</a></li>\n      <li><a href=\"https://www.bilibili.com/read/cv32462618/\">服务器部署开源大模型完整教程 Ollama+Gemma+open-webui - 哔哩哔哩 (bilibili.com)</a></li>\n      <li><a href=\"https://zhuanlan.zhihu.com/p/686952702\">Ollama管理本地开源大模型，用Open WebUI访问Ollama接口 - 知乎 (zhihu.com)</a></li>\n      <li><a href=\"https://zhuanlan.zhihu.com/p/672400265\">22K star的超强工具：Ollama，一条命令在本地跑 Llama2 - 知乎 (zhihu.com)</a></li>\n      <li><a href=\"https://zhuanlan.zhihu.com/p/648774481\">LLaMa-1 技术详解 - 知乎 (zhihu.com)</a></li>\n      <li><a href=\"https://wiki.eryajf.net/pages/97047e/#%E6%A8%A1%E5%9E%8B%E7%AE%A1%E7%90%86\">带你认识本地大语言模型框架Ollama(可直接上手) | 二丫讲梵 (eryajf.net)</a></li>\n      <li><a href=\"https://www.bytenote.net/article/225623142184255489\">https://www.bytenote.net/article/225623142184255489</a></li>\n      <li><a href=\"https://zhuanlan.zhihu.com/p/687099148\">https://zhuanlan.zhihu.com/p/687099148</a></li>\n    </ul>\n  </body>\n</html>\n"}},"__N_SSG":true}