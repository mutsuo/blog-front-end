<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><link rel="shortcut icon" href="/favicon.svg" type="image/x-icon"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><meta name="description" content="My portfolio website contains blogs and projects."/><title>Linux快速部署大语言模型LLaMa3，Web可视化j交互（Ollama+Open Web UI）</title><meta name="next-head-count" content="7"/><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin /><link rel="preload" href="/_next/static/css/220cfd29655cb454.css" as="style"/><link rel="stylesheet" href="/_next/static/css/220cfd29655cb454.css" data-n-g=""/><link rel="preload" href="/_next/static/css/70d615467024f80e.css" as="style"/><link rel="stylesheet" href="/_next/static/css/70d615467024f80e.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script src="/_next/static/chunks/webpack-7ee66019f7f6d30f.js" defer=""></script><script src="/_next/static/chunks/framework-db825bd0b4ae01ef.js" defer=""></script><script src="/_next/static/chunks/main-c841d66839f82a97.js" defer=""></script><script src="/_next/static/chunks/pages/_app-4ed25adc05e9a420.js" defer=""></script><script src="/_next/static/chunks/pages/blog/%5Bid%5D-151d326cfc654b68.js" defer=""></script><script src="/_next/static/MTcPhDYHURQpUR37d0Wae/_buildManifest.js" defer=""></script><script src="/_next/static/MTcPhDYHURQpUR37d0Wae/_ssgManifest.js" defer=""></script><style data-href="https://fonts.googleapis.com/css2?family=Cedarville+Cursive&family=Inter:wght@300;400;500;600&display=swap">@font-face{font-family:'Cedarville Cursive';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/cedarvillecursive/v17/yYL00g_a2veiudhUmxjo5VKkoqA-B_neJg.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcCO3FwrK3iLTeHuS_nVMrMxCp50SjIw2boKoduKmMEVuOKfMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcCO3FwrK3iLTeHuS_nVMrMxCp50SjIw2boKoduKmMEVuLyfMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcCO3FwrK3iLTeHuS_nVMrMxCp50SjIw2boKoduKmMEVuI6fMZs.woff) format('woff')}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcCO3FwrK3iLTeHuS_nVMrMxCp50SjIw2boKoduKmMEVuGKYMZs.woff) format('woff')}@font-face{font-family:'Cedarville Cursive';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/cedarvillecursive/v17/yYL00g_a2veiudhUmxjo5VKkoqA-B_nuIrpw4cNOTw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:300;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:400;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:500;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2JL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0460-052F,U+1C80-1C88,U+20B4,U+2DE0-2DFF,U+A640-A69F,U+FE2E-FE2F}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa0ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0301,U+0400-045F,U+0490-0491,U+04B0-04B1,U+2116}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2ZL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+1F00-1FFF}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa1pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0370-0377,U+037A-037F,U+0384-038A,U+038C,U+038E-03A1,U+03A3-03FF}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa2pL7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0102-0103,U+0110-0111,U+0128-0129,U+0168-0169,U+01A0-01A1,U+01AF-01B0,U+0300-0301,U+0303-0304,U+0308-0309,U+0323,U+0329,U+1EA0-1EF9,U+20AB}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa25L7W0Q5n-wU.woff2) format('woff2');unicode-range:U+0100-02AF,U+0304,U+0308,U+0329,U+1E00-1E9F,U+1EF2-1EFF,U+2020,U+20A0-20AB,U+20AD-20C0,U+2113,U+2C60-2C7F,U+A720-A7FF}@font-face{font-family:'Inter';font-style:normal;font-weight:600;font-display:swap;src:url(https://fonts.gstatic.com/s/inter/v18/UcC73FwrK3iLTeHuS_nVMrMxCp50SjIa1ZL7W0Q5nw.woff2) format('woff2');unicode-range:U+0000-00FF,U+0131,U+0152-0153,U+02BB-02BC,U+02C6,U+02DA,U+02DC,U+0304,U+0308,U+0329,U+2000-206F,U+2074,U+20AC,U+2122,U+2191,U+2193,U+2212,U+2215,U+FEFF,U+FFFD}</style></head><body><div id="__next"><div class="layout" color-mode="light"><main><div class="Nav_container__JsliS"><a class="Nav_signature__IYph1" href="/">Janborn</a><nav class="Nav_nav__Bx_p6"><span class="iconfont Nav_mobile__OanKb Nav_icon__7rlA4"></span><span class="iconfont Nav_mobile__OanKb Nav_icon__7rlA4"></span><a class="Nav_link__xIf3s Nav_laptop__CE709" href="/">关于我</a><a class="Nav_link__xIf3s Nav_laptop__CE709" href="/blog/">博文</a><a class="Nav_link__xIf3s Nav_laptop__CE709" href="/project/">做的东西</a><a href="https://github.com/mutsuo" rel="noreferrer" target="_blank" class="iconfont Nav_icon__7rlA4"></a><span class="iconfont Nav_icon__7rlA4"></span></nav></div><div class="Wrapper_wrapper__lMm8J"><p class="Markdown_title__o8uH2">Linux快速部署大语言模型LLaMa3，Web可视化j交互（Ollama+Open Web UI）</p><p class="Markdown_date__HjQt2">April 26, 2024</p><article class="md"><div><!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
    <h1>介绍</h1>
    <p>本文将介绍使用开源工具Ollama(60.6k⭐)部署LLaMa大模型，以及使用Open WebUI搭建前端Web交互界面的方法。</p>
    <p>我们先来过一遍几个相关的概念，对这块比较熟悉的朋友可跳过。</p>
    <h2>大规模语言模型</h2>
    <p>大规模语言模型（Large Language Models, LLMs），顾名思义是指在大量语料数据的基础上训练成的模型，能够模拟人类的语言风格生成较为生动的文本。这类模型的主要特征有：</p>
    <ul>
      <li>规模大：训练所使用的数据量非常庞大，有时超过1000亿个参数。</li>
      <li>复杂性高：模型结构比较复杂</li>
      <li>具有较好的上下文理解能力：大规模语言模型可以理解文本的上下文和细微差别</li>
    </ul>
    <h2>LLaMa</h2>
    <p>LLaMA是一种大规模语言模型，由Meta AI基于Transformer深度学习框架开发。该模型旨在生成各种风格的高质量文本（例如创意写作、对话甚至诗歌），能够胜任以下工作：</p>
    <ul>
      <li>自然语言处理（NLP）：理解和生成自然语言。</li>
      <li>机器学习：根据数据和算法学习新的信息和技能。</li>
      <li>对话生成：可以与用户进行对话，并根据情况生成合适的回应。</li>
    </ul>
    <h2>Ollama</h2>
    <blockquote>
      <p>官网：<a href="https://ollama.com/">Ollama</a></p>
      <p>API文档：<a href="https://github.com/ollama/ollama/blob/main/docs/api.md">ollama/docs/api.md at main · ollama/ollama (github.com)</a></p>
      <p>支持的模型列表：<a href="https://ollama.com/library">library</a></p>
    </blockquote>
    <p>一款可以快速部署大模型的工具。</p>
    <h2>Open WebUI</h2>
    <blockquote>
      <p>官网：<a href="https://openwebui.com/">Open WebUI</a></p>
      <p>相关介绍及源码：<a href="https://github.com/open-webui/open-webui">open-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI) (github.com)</a></p>
    </blockquote>
    <p>Open WebUI 是一个可视化的Web交互环境，它拥有清新简约的UI风格，具有可扩展、功能丰富、用户友好、自托管的特点，可以完全离线运行。它支持各种 LLM 运行程序，包括 Ollama 和 OpenAI 兼容的 API。</p>
    <h1>部署服务</h1>
    <p>本文介绍的方法使用于Linux系统，同样适用于Windows系统的WSL（安装方法可参见我的<a href="https://blog.csdn.net/mustuo/article/details/133960230?">这篇文章</a>）。</p>
    <h2>部署Ollama</h2>
    <p>1、下载Ollama</p>
    <p>Linux系统的安装命令如下：</p>
    <pre><code class="hljs language-shell">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
    <p>※此外<a href="https://ollama.com/download">官方</a>还提供了macOS和Windows的下载方式。</p>
    <p>2、下载llama3模型</p>
    <pre><code class="hljs language-shell">ollama run llama3
</code></pre>
    <p>※在<a href="https://ollama.com/blog/llama3">这里</a>可以看到该命令的相关介绍。</p>
    <p>上述命令将自动拉取模型，并进行sha256验签。处理完毕后自动进入llama3的运行环境，可以使用中文或英文进行提问，ctrl+D退出。</p>
    <p>3、配置服务</p>
    <p>为使外网环境能够访问到服务，需要对HOST进行配置。</p>
    <p>打开配置文件：<code>vim /etc/systemd/system/ollama.service</code>，根据情况修改变量<code>Environment</code>：</p>
    <ul>
      <li>服务器环境下：<code>Environment="OLLAMA_HOST=0.0.0.0:11434"</code></li>
      <li>虚拟机环境下：<code>Environment="OLLAMA_HOST=服务器内网IP地址:11434"</code></li>
    </ul>
    <p>3、启动服务</p>
    <p>启动服务的命令：<code>ollama serve</code></p>
    <p>首次启动可能会出现以下两个提示：</p>
    <blockquote>
      <p>Couldn't find '/home/用户名/.ollama/id_ed25519'. Generating new private key.</p>
    </blockquote>
    <p>该提示表示文件系统中不存在ssh私钥文件，此时命令将自动帮我们生成该文件，并在命令行中打印相应的公钥。</p>
    <blockquote>
      <p>Error: listen tcp 127.0.0.1:11434: bind: address already in use</p>
    </blockquote>
    <p>看到该提示，大概率服务已在运行中，可以通过<code>netstat -tulpn | grep 11434</code>命令进行确认。</p>
    <ul>
      <li>若命令输出的最后一列包含“ollama”字样，则表示服务已启动，无需做额外处理。</li>
      <li>否则，可尝试执行下列命令重启ollama：</li>
    </ul>
    <pre><code class="hljs language-shell"><span class="hljs-meta prompt_"># </span><span class="bash">ubuntu/debian</span>
sudo apt update
sudo apt install lsof
stop ollama
lsof -i :11434
kill &#x3C;PID>
ollama serve<span class="hljs-meta prompt_">

# </span><span class="bash">centos</span>
sudo yum update
sudo yum install lsof
stop ollama
lsof -i :11434
kill &#x3C;PID>
ollama serve
</code></pre>
    <p>如果您使用的是MacOS，可在<a href="https://github.com/ollama/ollama/issues/707">🔗这里</a>找到解决方法。</p>
    <p>4、在外网环境验证连接</p>
    <p>方法一：执行<code>curl http://ip:11434</code>命令，若返回“Ollama is running”，则表示连接正常。</p>
    <p>方法二：在浏览器访问<a href="http://ip:11434%EF%BC%8C%E8%8B%A5%E9%A1%B5%E9%9D%A2%E6%98%BE%E7%A4%BA%E6%96%87%E6%9C%AC%E2%80%9COllama">http://ip:11434，若页面显示文本“Ollama</a> is running”，则表示连接正常。</p>
    <h2>常用命令</h2>
    <p>1、进入llama3运行环境：<code>ollama run llama3</code></p>
    <p>2、启动服务：<code>ollama serve</code></p>
    <p>3、重启ollama</p>
    <pre><code class="hljs language-shell">systemctl daemon-reload
systemctl restart ollama
</code></pre>
    <p>4、重启ollama服务</p>
    <pre><code class="hljs language-shell"><span class="hljs-meta prompt_"># </span><span class="bash">ubuntu/debian</span>
sudo apt update
sudo apt install lsof
stop ollama
lsof -i :11434
kill &#x3C;PID>
ollama serve<span class="hljs-meta prompt_">

# </span><span class="bash">centos</span>
sudo yum update
sudo yum install lsof
stop ollama
lsof -i :11434
kill &#x3C;PID>
ollama serve
</code></pre>
    <p>5、确认服务端口状态：<code>netstat -tulpn | grep 11434</code></p>
    <h1>部署Open WebUI</h1>
    <p>1、下载Open WebUI</p>
    <p>Open WebUI基于docker部署，docker的安装方法可以参考<a href="https://zhuanlan.zhihu.com/p/651148141">这篇知乎文章</a>。</p>
    <p>Open WebUI既可以部署在服务端，也可以部署在客户端：</p>
    <pre><code class="hljs language-shell"><span class="hljs-meta prompt_"># </span><span class="bash">若部署在客户端，执行：</span>
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main<span class="hljs-meta prompt_">

# </span><span class="bash">若部署在服务端，执行：</span>
docker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</code></pre>
    <p>如果您的机器在国内，建议将<code>--restart</code>的参数值替换为<code>ghcr.nju.edu.cn/open-webui/open-webui:main</code>，下载速度会快非常多（见up主小杨生存日记的<a href="https://www.bilibili.com/read/cv32462618/">这篇文章</a>）。</p>
    <p>2、检查相关配置</p>
    <p>下载完之后，就可以在浏览器访问了，地址为<code>http://loacalhost:3000</code>（客户端部署）或<code>http://服务器ip:3000</code>。</p>
    <p>页面加载完成后（这个过程可能需要一些时间），新注册一个账号并登录。</p>
    <p>登录之后，点击页面顶端的齿轮⚙图标进入设置：</p>
    <ol>
      <li>侧边导航栏-General，将语言设置为中文</li>
      <li>侧边导航栏-连接，若“Ollama 基础 URL”这一项为<code>http://host.docker.internal:11434</code>，则表示ollama服务正常且连接成功；如果是空的，则需要回头检查一下ollama服务了</li>
      <li>侧边导航栏-模型，一般会自动拉取ollama服务上部署好的模型，可选模型参看<a href="https://ollama.com/library/llama3">官方的这篇文档</a></li>
      <li>其它的项目根据需要设置即可</li>
    </ol>
    <p>3、选择模型</p>
    <p>在顶端下拉框选择好模型，就可以开始提问啦！</p>
    <h1>参考文章</h1>
    <ul>
      <li><a href="https://juejin.cn/post/7359470175761350690">macOS + Ollama + Enchanted，本地部署最新 Llama3 - 掘金 (juejin.cn)</a></li>
      <li><a href="https://www.bilibili.com/read/cv32462618/">服务器部署开源大模型完整教程 Ollama+Gemma+open-webui - 哔哩哔哩 (bilibili.com)</a></li>
      <li><a href="https://zhuanlan.zhihu.com/p/686952702">Ollama管理本地开源大模型，用Open WebUI访问Ollama接口 - 知乎 (zhihu.com)</a></li>
      <li><a href="https://zhuanlan.zhihu.com/p/672400265">22K star的超强工具：Ollama，一条命令在本地跑 Llama2 - 知乎 (zhihu.com)</a></li>
      <li><a href="https://zhuanlan.zhihu.com/p/648774481">LLaMa-1 技术详解 - 知乎 (zhihu.com)</a></li>
      <li><a href="https://wiki.eryajf.net/pages/97047e/#%E6%A8%A1%E5%9E%8B%E7%AE%A1%E7%90%86">带你认识本地大语言模型框架Ollama(可直接上手) | 二丫讲梵 (eryajf.net)</a></li>
      <li><a href="https://www.bytenote.net/article/225623142184255489">https://www.bytenote.net/article/225623142184255489</a></li>
      <li><a href="https://zhuanlan.zhihu.com/p/687099148">https://zhuanlan.zhihu.com/p/687099148</a></li>
    </ul>
  </body>
</html>
</div></article><div class="Cd_cd__PXFYG"><a class="Cd_link__C6Mvd" href="/blog/">cd..</a></div><div class="Footer_footer__Tl1eP"><p class="Footer_cc__2oaAG"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noreferrer" target="_blank">CC BY-NC-SA 4.0</a> <!-- -->© 睦生</p><a href="https://beian.miit.gov.cn/" rel="noreferrer" target="_blank">闽ICP备2024038150号</a></div></div></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"data":{"id":"在Linux系统部署Ollama","date":"April 26, 2024","title":"Linux快速部署大语言模型LLaMa3，Web可视化j交互（Ollama+Open Web UI）","htmlContent":"\u003c!doctype html\u003e\n\u003chtml lang=\"en\"\u003e\n  \u003chead\u003e\n    \u003cmeta charset=\"utf-8\"\u003e\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ch1\u003e介绍\u003c/h1\u003e\n    \u003cp\u003e本文将介绍使用开源工具Ollama(60.6k⭐)部署LLaMa大模型，以及使用Open WebUI搭建前端Web交互界面的方法。\u003c/p\u003e\n    \u003cp\u003e我们先来过一遍几个相关的概念，对这块比较熟悉的朋友可跳过。\u003c/p\u003e\n    \u003ch2\u003e大规模语言模型\u003c/h2\u003e\n    \u003cp\u003e大规模语言模型（Large Language Models, LLMs），顾名思义是指在大量语料数据的基础上训练成的模型，能够模拟人类的语言风格生成较为生动的文本。这类模型的主要特征有：\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e规模大：训练所使用的数据量非常庞大，有时超过1000亿个参数。\u003c/li\u003e\n      \u003cli\u003e复杂性高：模型结构比较复杂\u003c/li\u003e\n      \u003cli\u003e具有较好的上下文理解能力：大规模语言模型可以理解文本的上下文和细微差别\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003eLLaMa\u003c/h2\u003e\n    \u003cp\u003eLLaMA是一种大规模语言模型，由Meta AI基于Transformer深度学习框架开发。该模型旨在生成各种风格的高质量文本（例如创意写作、对话甚至诗歌），能够胜任以下工作：\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e自然语言处理（NLP）：理解和生成自然语言。\u003c/li\u003e\n      \u003cli\u003e机器学习：根据数据和算法学习新的信息和技能。\u003c/li\u003e\n      \u003cli\u003e对话生成：可以与用户进行对话，并根据情况生成合适的回应。\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003ch2\u003eOllama\u003c/h2\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003e官网：\u003ca href=\"https://ollama.com/\"\u003eOllama\u003c/a\u003e\u003c/p\u003e\n      \u003cp\u003eAPI文档：\u003ca href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\"\u003eollama/docs/api.md at main · ollama/ollama (github.com)\u003c/a\u003e\u003c/p\u003e\n      \u003cp\u003e支持的模型列表：\u003ca href=\"https://ollama.com/library\"\u003elibrary\u003c/a\u003e\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003e一款可以快速部署大模型的工具。\u003c/p\u003e\n    \u003ch2\u003eOpen WebUI\u003c/h2\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003e官网：\u003ca href=\"https://openwebui.com/\"\u003eOpen WebUI\u003c/a\u003e\u003c/p\u003e\n      \u003cp\u003e相关介绍及源码：\u003ca href=\"https://github.com/open-webui/open-webui\"\u003eopen-webui/open-webui: User-friendly WebUI for LLMs (Formerly Ollama WebUI) (github.com)\u003c/a\u003e\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003eOpen WebUI 是一个可视化的Web交互环境，它拥有清新简约的UI风格，具有可扩展、功能丰富、用户友好、自托管的特点，可以完全离线运行。它支持各种 LLM 运行程序，包括 Ollama 和 OpenAI 兼容的 API。\u003c/p\u003e\n    \u003ch1\u003e部署服务\u003c/h1\u003e\n    \u003cp\u003e本文介绍的方法使用于Linux系统，同样适用于Windows系统的WSL（安装方法可参见我的\u003ca href=\"https://blog.csdn.net/mustuo/article/details/133960230?\"\u003e这篇文章\u003c/a\u003e）。\u003c/p\u003e\n    \u003ch2\u003e部署Ollama\u003c/h2\u003e\n    \u003cp\u003e1、下载Ollama\u003c/p\u003e\n    \u003cp\u003eLinux系统的安装命令如下：\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003ecurl -fsSL https://ollama.com/install.sh | sh\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e※此外\u003ca href=\"https://ollama.com/download\"\u003e官方\u003c/a\u003e还提供了macOS和Windows的下载方式。\u003c/p\u003e\n    \u003cp\u003e2、下载llama3模型\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003eollama run llama3\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e※在\u003ca href=\"https://ollama.com/blog/llama3\"\u003e这里\u003c/a\u003e可以看到该命令的相关介绍。\u003c/p\u003e\n    \u003cp\u003e上述命令将自动拉取模型，并进行sha256验签。处理完毕后自动进入llama3的运行环境，可以使用中文或英文进行提问，ctrl+D退出。\u003c/p\u003e\n    \u003cp\u003e3、配置服务\u003c/p\u003e\n    \u003cp\u003e为使外网环境能够访问到服务，需要对HOST进行配置。\u003c/p\u003e\n    \u003cp\u003e打开配置文件：\u003ccode\u003evim /etc/systemd/system/ollama.service\u003c/code\u003e，根据情况修改变量\u003ccode\u003eEnvironment\u003c/code\u003e：\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e服务器环境下：\u003ccode\u003eEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"\u003c/code\u003e\u003c/li\u003e\n      \u003cli\u003e虚拟机环境下：\u003ccode\u003eEnvironment=\"OLLAMA_HOST=服务器内网IP地址:11434\"\u003c/code\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cp\u003e3、启动服务\u003c/p\u003e\n    \u003cp\u003e启动服务的命令：\u003ccode\u003eollama serve\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003e首次启动可能会出现以下两个提示：\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eCouldn't find '/home/用户名/.ollama/id_ed25519'. Generating new private key.\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003e该提示表示文件系统中不存在ssh私钥文件，此时命令将自动帮我们生成该文件，并在命令行中打印相应的公钥。\u003c/p\u003e\n    \u003cblockquote\u003e\n      \u003cp\u003eError: listen tcp 127.0.0.1:11434: bind: address already in use\u003c/p\u003e\n    \u003c/blockquote\u003e\n    \u003cp\u003e看到该提示，大概率服务已在运行中，可以通过\u003ccode\u003enetstat -tulpn | grep 11434\u003c/code\u003e命令进行确认。\u003c/p\u003e\n    \u003cul\u003e\n      \u003cli\u003e若命令输出的最后一列包含“ollama”字样，则表示服务已启动，无需做额外处理。\u003c/li\u003e\n      \u003cli\u003e否则，可尝试执行下列命令重启ollama：\u003c/li\u003e\n    \u003c/ul\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003e\u003cspan class=\"hljs-meta prompt_\"\u003e# \u003c/span\u003e\u003cspan class=\"bash\"\u003eubuntu/debian\u003c/span\u003e\r\nsudo apt update\r\nsudo apt install lsof\r\nstop ollama\r\nlsof -i :11434\r\nkill \u0026#x3C;PID\u003e\r\nollama serve\r\u003cspan class=\"hljs-meta prompt_\"\u003e\n\r\n# \u003c/span\u003e\u003cspan class=\"bash\"\u003ecentos\u003c/span\u003e\r\nsudo yum update\r\nsudo yum install lsof\r\nstop ollama\r\nlsof -i :11434\r\nkill \u0026#x3C;PID\u003e\r\nollama serve\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e如果您使用的是MacOS，可在\u003ca href=\"https://github.com/ollama/ollama/issues/707\"\u003e🔗这里\u003c/a\u003e找到解决方法。\u003c/p\u003e\n    \u003cp\u003e4、在外网环境验证连接\u003c/p\u003e\n    \u003cp\u003e方法一：执行\u003ccode\u003ecurl http://ip:11434\u003c/code\u003e命令，若返回“Ollama is running”，则表示连接正常。\u003c/p\u003e\n    \u003cp\u003e方法二：在浏览器访问\u003ca href=\"http://ip:11434%EF%BC%8C%E8%8B%A5%E9%A1%B5%E9%9D%A2%E6%98%BE%E7%A4%BA%E6%96%87%E6%9C%AC%E2%80%9COllama\"\u003ehttp://ip:11434，若页面显示文本“Ollama\u003c/a\u003e is running”，则表示连接正常。\u003c/p\u003e\n    \u003ch2\u003e常用命令\u003c/h2\u003e\n    \u003cp\u003e1、进入llama3运行环境：\u003ccode\u003eollama run llama3\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003e2、启动服务：\u003ccode\u003eollama serve\u003c/code\u003e\u003c/p\u003e\n    \u003cp\u003e3、重启ollama\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003esystemctl daemon-reload\r\nsystemctl restart ollama\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e4、重启ollama服务\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003e\u003cspan class=\"hljs-meta prompt_\"\u003e# \u003c/span\u003e\u003cspan class=\"bash\"\u003eubuntu/debian\u003c/span\u003e\r\nsudo apt update\r\nsudo apt install lsof\r\nstop ollama\r\nlsof -i :11434\r\nkill \u0026#x3C;PID\u003e\r\nollama serve\r\u003cspan class=\"hljs-meta prompt_\"\u003e\n\r\n# \u003c/span\u003e\u003cspan class=\"bash\"\u003ecentos\u003c/span\u003e\r\nsudo yum update\r\nsudo yum install lsof\r\nstop ollama\r\nlsof -i :11434\r\nkill \u0026#x3C;PID\u003e\r\nollama serve\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e5、确认服务端口状态：\u003ccode\u003enetstat -tulpn | grep 11434\u003c/code\u003e\u003c/p\u003e\n    \u003ch1\u003e部署Open WebUI\u003c/h1\u003e\n    \u003cp\u003e1、下载Open WebUI\u003c/p\u003e\n    \u003cp\u003eOpen WebUI基于docker部署，docker的安装方法可以参考\u003ca href=\"https://zhuanlan.zhihu.com/p/651148141\"\u003e这篇知乎文章\u003c/a\u003e。\u003c/p\u003e\n    \u003cp\u003eOpen WebUI既可以部署在服务端，也可以部署在客户端：\u003c/p\u003e\n    \u003cpre\u003e\u003ccode class=\"hljs language-shell\"\u003e\u003cspan class=\"hljs-meta prompt_\"\u003e# \u003c/span\u003e\u003cspan class=\"bash\"\u003e若部署在客户端，执行：\u003c/span\u003e\r\ndocker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\r\u003cspan class=\"hljs-meta prompt_\"\u003e\n\r\n# \u003c/span\u003e\u003cspan class=\"bash\"\u003e若部署在服务端，执行：\u003c/span\u003e\r\ndocker run -d -p 3000:8080 -e OLLAMA_BASE_URL=https://example.com -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n\u003c/code\u003e\u003c/pre\u003e\n    \u003cp\u003e如果您的机器在国内，建议将\u003ccode\u003e--restart\u003c/code\u003e的参数值替换为\u003ccode\u003eghcr.nju.edu.cn/open-webui/open-webui:main\u003c/code\u003e，下载速度会快非常多（见up主小杨生存日记的\u003ca href=\"https://www.bilibili.com/read/cv32462618/\"\u003e这篇文章\u003c/a\u003e）。\u003c/p\u003e\n    \u003cp\u003e2、检查相关配置\u003c/p\u003e\n    \u003cp\u003e下载完之后，就可以在浏览器访问了，地址为\u003ccode\u003ehttp://loacalhost:3000\u003c/code\u003e（客户端部署）或\u003ccode\u003ehttp://服务器ip:3000\u003c/code\u003e。\u003c/p\u003e\n    \u003cp\u003e页面加载完成后（这个过程可能需要一些时间），新注册一个账号并登录。\u003c/p\u003e\n    \u003cp\u003e登录之后，点击页面顶端的齿轮⚙图标进入设置：\u003c/p\u003e\n    \u003col\u003e\n      \u003cli\u003e侧边导航栏-General，将语言设置为中文\u003c/li\u003e\n      \u003cli\u003e侧边导航栏-连接，若“Ollama 基础 URL”这一项为\u003ccode\u003ehttp://host.docker.internal:11434\u003c/code\u003e，则表示ollama服务正常且连接成功；如果是空的，则需要回头检查一下ollama服务了\u003c/li\u003e\n      \u003cli\u003e侧边导航栏-模型，一般会自动拉取ollama服务上部署好的模型，可选模型参看\u003ca href=\"https://ollama.com/library/llama3\"\u003e官方的这篇文档\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e其它的项目根据需要设置即可\u003c/li\u003e\n    \u003c/ol\u003e\n    \u003cp\u003e3、选择模型\u003c/p\u003e\n    \u003cp\u003e在顶端下拉框选择好模型，就可以开始提问啦！\u003c/p\u003e\n    \u003ch1\u003e参考文章\u003c/h1\u003e\n    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"https://juejin.cn/post/7359470175761350690\"\u003emacOS + Ollama + Enchanted，本地部署最新 Llama3 - 掘金 (juejin.cn)\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://www.bilibili.com/read/cv32462618/\"\u003e服务器部署开源大模型完整教程 Ollama+Gemma+open-webui - 哔哩哔哩 (bilibili.com)\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/686952702\"\u003eOllama管理本地开源大模型，用Open WebUI访问Ollama接口 - 知乎 (zhihu.com)\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/672400265\"\u003e22K star的超强工具：Ollama，一条命令在本地跑 Llama2 - 知乎 (zhihu.com)\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/648774481\"\u003eLLaMa-1 技术详解 - 知乎 (zhihu.com)\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://wiki.eryajf.net/pages/97047e/#%E6%A8%A1%E5%9E%8B%E7%AE%A1%E7%90%86\"\u003e带你认识本地大语言模型框架Ollama(可直接上手) | 二丫讲梵 (eryajf.net)\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://www.bytenote.net/article/225623142184255489\"\u003ehttps://www.bytenote.net/article/225623142184255489\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/687099148\"\u003ehttps://zhuanlan.zhihu.com/p/687099148\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n"}},"__N_SSG":true},"page":"/blog/[id]","query":{"id":"在Linux系统部署Ollama"},"buildId":"MTcPhDYHURQpUR37d0Wae","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>